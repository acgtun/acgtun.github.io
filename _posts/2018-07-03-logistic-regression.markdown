---
layout: post
title: 'Logistic Regression'
date: '2018-07-03 13:36:00 -0400'
categories: deep learning
---

$$z=w^Tx+b$$

$$σ(z)=\frac{1}{1+e^{-z}}$$

$$\hat{y}=σ(z)$$

$$a=\hat{y}$$

Loss Function: $$L=-[\hat{y}log\hat{y}+(1-\hat{y})log(1-\hat{y})]$$

$$P(y=1 \vert x_i)=\sigma(z)=\hat{y}$$, then $$P(y=0 \vert x_i)=1-\sigma(z)=1-\hat{y}$$



$$a^2 + b^2 = c^2$$
